## 0. 前置き

### どんな資料か

- 強化学習の背景にある理論がなんとなくわかる
- 強化学習をこれから勉強していきたいときの指針になる
- この資料だけでなく、勉強になる強化学習関連の優良記事を紹介する

### 対象者

- ディープラーニングを用いた教師あり学習が若干でもわかる
- 強化学習をやってみたい

### その他

建前：ガチで勉強したい人は専門書を読むのが良いと思うので

本音：ちゃんと理論までカバーするのはつらいので

⇒厳密性は廃して書きました



## 1. 強化学習とは

前置きです。強化学習あまりわからない、という人は読んでください。

### 1-1 強化学習を使うと何ができるのか

強化学習の用途がイメージできていない方は[この記事](<https://ai-kenkyujo.com/2020/07/29/kyoukagakusyu/>)の2章までを見てください。

### 1-2 強化学習の定義

まずはいきなり**エージェント**と**環境**という概念を定義し、同時に関連する言葉も定義します。**エージェント**は行動の主体であり、**環境**はエージェントが存在する空間です（環境はエージェントを含むことが多いです、つまり環境 = エージェントの周囲の環境 + エージェント）。強化学習においては、エージェントと環境は基本的には次のような関係になければなりません。

- 環境はエージェントに対し、環境内に存在する情報の全てまたは一部を与えます。環境がエージェントに与える情報を**状態**（または**観測**とも）と言います。
- エージェントは環境から受け取った**状態**をもとに自らの取るべき行動を決定することができます。ここで決定する行動を（そのまま）**行動**と言います。
  - 可能な行動の集合（ようは行動の選択肢すべて）のことを**行動空間**と言います。
  - **ある状態においてどの行動をどの程度の確率で選択するか**というエージェントの行動原則のことを**方策**と呼びます（大事！）
- エージェントの行動の結果は環境に影響を与えます。
- 環境は行動の結果を何らかの方法で評価してエージェントにフィードバックします。ここでフィードバックされる情報を**報酬**と言います。
- 現在の状態の観測、行動、次の状態への遷移（、＋次の状態の観測）をセットにしたものを**ステップ**と言います。強化学習では、ステップを繰り返すことでエージェントが次の状態へと移行していく過程で学習を行います。

よくこんな感じで図解されます。（図は[ここ](http://takionaka.livedoor.blog/archives/4723105.html)のものを甲斐が勝手に編集したもの）

![](C:/Users/kazun/Desktop/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92/01-01.png)

ここで、優れたエージェントとは「状態に応じて、多くの報酬を得られる行動を選択する方策」を持ったエージェントを指すといえます。つまりこの問題のミソは、文字通り脳ミソである「方策」であるといえます。**強化学習**とはこの「方策」を学習によって作り上げていく手法です。

＜補足＞エージェントに実体がない場合（囲碁ゲームのAIなど）にはエージェントと方策は同一視できるので、その影響からか方策を指してエージェントという単語を使うことがあります。また、方策を指してAIと言うこともたまにあります。一般向けには問題ないですが、強化学習手法を語る場合はきちんと区別するのが無難でしょう（上の図は、エージェントを方策と同一視したうえで方策をAIと表現していますね。実際AIの定義は未だに揺れが大きいので、エージェントをAIと呼んでも文句は言えませんが…）。

### 1-3 ディープラーニングとの関連性

単に「強化学習」といった場合は慣例的に「ディープラーニングを用いた強化学習」を指すことも多いです。しかし、強化学習は必ずしもディープラーニングを用いるものではなく、ディープラーニングがメジャーになる前から使用されていました。後述する**Q学習**などはディープラーニングを用いなくても使える手法であり、現在でも強化学習のスタンダードな方法として紹介されることが多いです。

一方でディープラーニングが流行して以降、強化学習は急激な発展を見せました（2019年ころからは落ち着いていますが）。その理由は（甲斐の独自解釈ですが）次の2つです。

- Q学習の課題を解決するためにディープラーニングが有用であった
- 強化学習で用いる「方策を最適化するアルゴリズム」とディープラーニングとの親和性が凄まじかったため

### 1-4 教師あり学習・教師なし学習との関連性

よく「教師あり学習」「教師なし学習」と並立する第三の概念として解説されますが、無関係の技術と考えるほうが良いと思います。なぜなら、背景となる「古典的な」強化学習の手法が教師あり学習・教師なし学習とは全く異なるためです。

ただし、強化学習を学ぶ前に、**教師あり学習の知識は持っておくのが望ましい**です。なぜなら、強化学習で最適化することになる**方策** / **状態価値** / **行動価値** の学習にはいずれも教師あり学習とよく似た（同一視できるかも？）方法が使われるためです。特にディープラーニングを用いた場合のアルゴリズムは教師あり学習の理解が必要です。

### 1-5 表記

これまで定義した用語に対し、次のような表記を行うのが一般的です。

- 状態　s
- 行動　a
- 報酬　R

また、t 番目のステップを時間 t ということが一般的です。時間 t における上記3項目を下記のように表記します（本来 t は下付き文字）。

- 状態　st
- 行動　at
- 報酬　Rt

## 2. 強化学習の種類

### 2-1 離散行動空間と連続行動空間

#### 2-1-1 離散行動空間

**離散行動空間**とは、その要素である行動が離散的（連続的でないこと）であるような行動空間を指します。離散行動空間を持つエージェントは、状態から各行動の評価値を計算し、評価値に基づき行動を選択します（分類問題のような要領です）。

離散行動空間の例として、下記のようなものがあります。

- cart-poleタスク：「右に行くか」「左に行くか」「静止するか」が行動の選択肢である
- 囲碁AI：「19×19マスのうち何処に石を置くか」が行動の選択肢である
- 後述する「連続行動空間」を離散化した場合

#### 2-1-2 連続行動空間

一方、**連続行動空間**とは、その要素である行動が連続的であるような行動空間を指します。連続行動空間を持つエージェントは、状態から行動（連続値）を直接計算し、行動します（回帰問題のような要領です）。

連続行動空間の例として、下記のようなものがあります。

- pendulumタスク：「左右にどれだけのトルクをかけるか」の連続値を行動とする
- ロボットの制御：「目標となるxyz座標」や「各関節の角度」や「各関節のトルク」のような連続値を行動とする
- その他、現実に存在する多くのタスク

#### 2-1-3 勉強のための指針

まずは連続行動空間のことは忘れて、**離散行動空間のみを考える**ことにします。なぜならば、最も初歩的なQ学習やSARSA以外の手法は基本的に離散行動空間と連続行動空間の両方に対応でき、かつ離散行動空間には下記のような利点があるためです。

- 離散行動空間のほうが問題が単純なものが多く、初学者に向く
- 学習の結果としてどのような行動を選択したか可視化しやすい
- ディープラーニングのような複雑な学習機を使わなくても可能

### 2-2 価値ベースと方策ベース

前述のとおり、離散行動空間を考えます。行動の選択肢が10個あり、この中から良い行動を選びたいと考えた場合、どのように決定しようとするでしょうか？

#### 2-2-1 方策ベース

1つの立場は、行動の選択肢ごとに「どの程度の報酬が得られそうか」を計算し、最も得られる報酬の大きそうなものを選ぶというものです。この方法は行動を選択する方策にフォーカスを当てた方法であるため、**方策ベース**の方法と言われます。

#### 2-2-2 価値ベース

もう1つの立場は、行動ではなく状態が評価値（**状態価値**と言います）を持ち、各行動の結果として得られる「次の状態の状態価値」と「即座に得られる報酬」の和が最も大きくなるような行動を選ぶというものです。この方法は状態価値にフォーカスを当てた方法であるため、**価値ベース**の方法と言われます。

＜補足＞「次の状態の価値」は行動してみないとわからない、と言いたいところなのですが、**行動状態価値**と**ベルマン方程式**の導入によって解決します。行動状態価値は、ある状態においてある行動をとった場合の「次の状態の状態価値の期待値」を表し、その最大値（行動に関する最大値）が状態価値であると定義します。ベルマン方程式は現在の状態価値と次の時間での状態価値を結びつけた式です。この2つを組み合わせて式展開することで、「現在ある行動をとった場合の、次の時間の行動状態価値」が「現在の行動状態価値」から計算できてしまうことが示されているのです。行動状態価値は**とても頭の良い人がとても都合の良いように定義してくれた**ので、この議論は結局、前述の『「次の状態の状態価値」と「即座に得られる報酬」の和』を最大にするためには行動状態価値を最大にする行動を選択すればよい、という結論に着地します（この辺はちゃんと式を見たほうがわかりやすいです）。

#### 2-2-3 比較

方策ベースの手法も価値ベースの手法も、行動の選択肢ごとに何かの評価値を計算し最大のものを選ぶという点で似たような外見に落ち着きます。しかしその中身の考え方や計算方法、学習方法などは別物と考えて良いでしょう。

中身の異なる2つの手法ですが、それぞれに特有のメリットがあることから、どちらかが切り捨てられることなくほぼ並立した状態で研究が進められてきました。

方策ベースの方法は直感的でありながら極めて強力であり、ロボットの制御など行動の次元（ロボットでいえば各ジョイントの動きなど）が大きいタスクにおいて良い結果を残しています。

一方、価値ベースの方法は回りくどいですがこちらも強力であり、Atariのゲームなど行動の次元が小さいタスクでは方策ベースの方法を上回るパフォーマンスを発揮することが多いです。

また、Actor-Criticと呼ばれる強化学習手法はこれら2つの方法を融合したものであり、こちらも優秀な結果を残しています。

#### 2-2-4 勉強のための指針

価値ベースの手法、方策ベースの**両方を勉強する**ことをお勧めします。2つの手法でともに異なるメリットがあるため、またそのハイブリッドであるActor-Criticが強力であるためです。

その中でも最初に勉強すべきものとして、まずは**Q学習**ををお勧めします。Q学習については、次章で詳しく解説します。

### 2-3 おまけ：強化学習手法マップ

こんな感じの枠組みで理解するとわかりやすいかと思います。なお、Actor-Criticは方策ベースの手法に価値ベースの方法を付け足したという意味合いが強いので、方策ベースの一種として整理しています。

![強化学習マップ](強化学習マップ.png)

また、[こちら](<https://qiita.com/shionhonda/items/ec05aade07b5bea78081#actor-critic%E7%B3%BB>)の記事もわかりやすいです。わかりやすいほうで理解していただければ大丈夫です。

## 3. Q学習

### 3-1 Q学習の特徴

改めてQ学習の特徴を整理すると下記のようになります。

- ディープラーニング以前から使用されている強化学習手法
- **価値ベース**手法の一種
  - その中でも**TD学習**（⇒3-2-4項）という方法を拡張したもの
- **方策オフ**の手法（⇒？？？項）
- **離散行動空間**のタスクしか解けない

Q学習には単純ゆえに次のような利点・欠点があります。

- 利点

  - 行動価値関数が「表」「行列」「テンソル」のような形で表されるため、行動価値が更新されていく様子が簡単に理解できる
  - 価値ベースの手法の学習の基本である**ベルマン更新**（⇒3-2-3項）の式をそのまま用いるため、入門用に良い（わかりやすいとは言ってない）

  - cart-poleやmountain_carのような程度の簡単なタスクなら、下手にDQNなどを使うより早く収束するうえ、マシンパワーが必要ない

- 欠点

  - 行動価値関数を「表」「行列」「テンソル」のような形で保持するため、状態や行動空間が複雑になるほど不利
    - メモリを食うため
    - 全条件を探索できるまでに莫大な試行回数が必要
  - ディープラーニングの恩恵を受けられない
  - 離散行動空間のタスクしか解けない

### 3-2 前提知識の紹介

ここではQ学習の前提となる知識を紹介していきます。この資料はぼんやりとしたまとめしか書いていません。各項ごとに参考資料として挙げた資料を順番に読んでいくと前提知識が得られるように作っていますので、そちらをメインに読んでください。

#### 3-2-1 報酬、リターン

- 報酬：ある状態からある行動をした結果、環境から得られる何らかの[点数、評価値、などなど]
  - 即時報酬：「報酬」のなかでも、「現在の状態から次の状態へ移行したときに得られる報酬」というニュアンスを強調したいときに用いられる言葉
  - 長期報酬：「報酬」のなかでも、「即時報酬以外のもの」「即時報酬 + 即時報酬以外のもの」というニュアンスを強調したいときに用いられる言葉
- リターン：「ある状態からある行動をした結果、エピソード終了までに得られる報酬の総和」の期待値
  - ただし、1ステップ後に行くたびに割引率γ（ガンマ）を乗算する
    - 即時報酬だけでなく長期報酬を考慮することができる。長期報酬をどの程度重視するかは割引率γで調整可能（1：即時報酬と同等に評価する、0：長期報酬を評価しない）

#### 3-2-2 状態価値

価値ベースの手法が「価値ベース」たる所以が**状態価値**の概念です。2-2節で述べた通り、価値ベースの手法では、次のような理念を数式化します。

- 行動ではなく状態が価値を持つ
  - 状態 s にあるときの状態価値 V を V(s) と表す
- 状態価値とは、その状態から行動を繰り返した場合のリターンの期待値である
- 行動は何らかの固定された方策に基づいて選択される。例えば、「可能な行動から一様ランダムに決定する」など。

＜参考＞

- [今さら聞けない強化学習（1）：状態価値関数とBellman方程式](<https://qiita.com/triwave33/items/5e13e03d4d76b71bc802>)（前半）

#### 3-2-3 状態価値に関するベルマン方程式

ベルマン方程式は**価値ベースの手法の基礎**となる式です。ただし、完璧に覚えている必要はなく、なんとなく存在を知っているくらいで問題ありません。

（状態価値に関する）ベルマン方程式の前提は「状態の持つ価値とは、その状態から行動を繰り返した場合のリターンの期待値である」（3-2-1項）という点です。これを定式化して変形することで、V(st) を V(st+1) と即時報酬を用いて漸化式のように表記できることを偉い人が発見しました。これがベルマン方程式です。

＜参考＞

- [今さら聞けない強化学習（1）：状態価値関数とBellman方程式](<https://qiita.com/triwave33/items/5e13e03d4d76b71bc802#%E4%BE%A1%E5%80%A4%E3%81%AE%E5%AE%9A%E9%87%8F%E5%8C%96>)（後半）

#### 3-2-4 TD学習

状態価値を理解すればTD学習が可能になります（いよいよ学習です）。学習というだけあって、TD学習では初めから正しい V(s) が推定できることを期待しません。代わりに、V(s) を正しいと思われる方向に少しずつ更新していくことで、何度もエピソードを繰り返すうちに正しい V(s) に近い値が得られることを期待します。

ここでベルマン方程式を応用します。ベルマン方程式は正しい V(st) とV(st+1) の組において成立する式であり、当然学習初期には成立しません。しかし、V(st) をベルマン方程式から期待される値に近づくように更新していくことで、徐々に正しい値に近づけるという操作により、V(st) が正しい値に収束するよう学習できるということが示されています。この操作を**ベルマン更新**と言います。

ベルマン更新では、V(st+1) と即時報酬 γt からベルマン方程式を成立させるような V(st) の値を計算し、元々の V(st) の値が少しだけそれに近づくように更新します。各ステップごとにベルマン更新を行い V を更新していく学習方法をTD学習とよびます。

＜参考＞

- [今さら聞けない強化学習（9）: TD法の導出](<https://qiita.com/triwave33/items/277210c7be4e47c28565#td%E6%B3%95>)（後半部分のみ理解すればOKです）

#### 3-2-5 行動状態価値

より高度なアルゴリズムであるQ学習、DQNへとステップアップしていくために、状態価値を拡張した**行動状態価値** Q(s, a) を導入します。

状態価値 V(s) が「状態 s から方策に基づいて行動を繰り返した場合のリターンの期待値」を表すのに対し、行動状態価値 Q(s, a) は「状態 s において行動 a を選択し、その後は方策に基づいて行動を繰り返した場合のリターンの期待値」を表します。

＜参考＞

- [今さら聞けない強化学習（3）：行動価値関数とBellman方程式](<https://qiita.com/triwave33/items/8966890701169f8cad47>)（前半）

#### 3-2-6 行動状態価値に関するベルマン方程式

行動状態価値 Q(s, a) には、「方策に基づいた場合に行動 a が選択される確率（よくπ(s, a)と書かれます）を用いて Q(s, a) をすべての行動 a について重み付き平均すると V(s)と等しくなる」という重要な性質があります。

これを3-2-3項のベルマン方程式に代入して変形することで、 Q(s, a) についても、Q(st, at) と Q(st+1, at+1) との関係を定式化することができます。

＜参考＞

- [今さら聞けない強化学習（3）：行動価値関数とBellman方程式](<https://qiita.com/triwave33/items/8966890701169f8cad47>)（後半）

### 3-3 Q学習のゴール

Q学習はその名の通り、行動状態価値 Q(s, a) を学習していく手法です。ここでは次の仮定をします。

- 状態 s は離散値、つまりいくつかある状態のうちの1つ、という状態を取る
  - また、このとき変数が1つしかないという意味で s は1次元である
- 行動 a は離散値、つまりいくつかある行動の選択肢から1つを選ぶ
  - また、このとき決定すべき a が1つしかない という意味で a は1次元である

このとき、Q(s, a) は単なる二次元の行列に数値が詰まったものであるといえます。つまり、この行列は**「s のとき a したら報酬の総和は推定これくらいだよ」ということがわかる表**になっているということです（s や a が1次元でないタスクの場合もこの行列を高次元に拡大するだけです）。これを学習によって得ることができれば、ある状態 s において最も多くのリターンが得られそうな行動を選び続けることができます。これを得るのがQ学習の目的です。

なお、「Q(s, a) を求めて a を選択する」という意思決定方法は、DQNなど他の価値ベースの手法でも（表ではなく関数という形式を用いる場合がほとんどですが）共通です。

### 3-4 Q学習をしてみる









a